<html>
<head><title>All Blog Posts</title></head>
<body>
<h1>All Blog Posts</h1>
<article>
<main>
        <h1>A 'Sliding Doors' moment, or is it?</h1>
        <p>This is not a tale of a pivotal moment and then the after effects.
It is not the point where I talk about an important decision I had to make and I could foresee some things that may follow on should I make the decision one way, and then what may happen if I made the decision the other way.  </p>

<p>These are situations where you know you stand before something momentous and are weighing up the pros and cons of choosing one option over others.  We could call this a quandary.</p>

<p>The thing is, this is not what is meant by a <a href="https://neuroalchemist.com/sliding-door/">Sliding Doors moment</a>.  The difference is quite striking because in the above <em>quandary</em> situation we <strong>know</strong> we have a big decision to make; we <strong>know</strong> there will be sizable repercussions following our decision.  </p>

<blockquote>
  <p>We are fully aware of the gravity of the situation we find ourself in</p>
</blockquote>

<p>However, a <em>Sliding Doors moment</em> is one where we are unaware we are at a pivotal juncture, that our fate will change after this moment.  We are just <em>being</em> or <em>doing</em> and only later can we look back and attribute the fact some later effect occurred to an innocuous decision we made earlier.  </p>

<p>The sliding doors moment in the film that originates the term is whether the character boards a train.  The film then shows two different life paths based on boarding and not boarding the train. Boarding the train meant the character arrived home in time to catch her cheating partner, which led to other decisions and a prosperous life.  Missing the train caused the character to arrive home too late to witness the infidelity, and then living an unhappy and less prosperous life with that partner. </p>

<p>There was no conscious decision to <em>not</em> board the train but the character was merely too late to make it onto the train.  This event would have been caused by earlier decisions, leading to the character arriving at the station too late.  So, again, the character did not <em>knowingly</em> stand before a turning point in her life.  She did not have a quandary whether to or not to catch the train.  It was just fate, one could say.</p>

<p>I like the Zig Ziglar quote on this, taken from the above-quoted article:</p>

<blockquote>
  <p>Your life is a result of the choices you have made. If you don’t like your life, start making better choices.<em> - Zig Ziglar</em></p>
</blockquote>

<p>So, in likening our current situation to the sliding doors moment, often we are merely saying, <em>"I have a tough decision to make as either of the options I choose could lead me into the unknown, and I find this scary!"</em></p>

</main>
</article>
<article>
<main>
        <h1>Counterfactual thinking: good and bad for us</h1>
<p>In thinking about decisions while writing the <a href="a-sliding-doors-moment-or-is-it.html">Sliding Doors moment post</a> it occurred to me that in mislabeling conscious turning points in one's life as a sliding doors moment we are usually weighing up pros and cons, or risks and rewards to taking a certain action.  We are looking into the perceived future and reflecting on how we may feel once the new reality hits us.  In other words, we are thinking, "will I regret this decision?".</p>

<p>This is when we begin to ruminate about what might have happened had we not taken <em>this</em> option but rather <em>that</em> option.  So, we are simulating the upsides and, more likely, the downsides of particular options.</p>

<p>In my mind, if you engage in this kind of thinking then you are also likely to be the kind of person who often looks back at situations with regret or "if only" thoughts.</p>

<p>This, like most concepts, has a defined term for it: <em>Counterfactual thinking</em>. </p>

<p>Counterfactual thinking is where we look back to the past and think, "I wish I hadn't done that", or "I'm glad I made that decision".</p>

<p>I am 100% in line with the second example; I never have regrets and believe I did the best I could with the knowledge, skills, time, and facilities I had <em>at the time</em>.
The important part, for me is: <strong>at the time</strong>. </p>

<p>We can all be wise after the event.  We all know what the best choice was once we've seen the light after the fact.</p>

<p>So, it turns out, I engage more in <em>Downward</em> counterfactual thinking, while the 'regretful' people focus more on <em>Upward</em> counterfactual thinking. 
The labeling of regretful and more negative thinking as <em>upward</em> seems counter-intuitive to me and confuses me each time I think about it.
Perhaps I need to research it more to discover why the terms were attributed this way around.</p>

<p>Why does any of this matter?</p>

<p>Well, there are positive effects of downward counterfactual thinking.  I won't go into those, though.</p>

<p>Plus, it is thought that upward counterfactual thinking is more associated with depression.  A study can be read <a href="https://www.sciencedirect.com/science/article/abs/pii/S0272735816301714">here</a>. </p>

<p>However, it isn't that simple.  Downward counterfactual thinking is not always good!
Some upward counterfactual thinking ("I wish had tried harder at work") is good.  If we take notice of it and learn from it, we may improve outcomes of similar situations in the future.  </p>

<p>So, I now have a name for what I've always been aware of about the way we look back on events and how we think about them.
I also have to be careful not to confuse <em>upward</em> (negative) with <em>downward</em> (positive)!</p>
</main>
</article>
<article>
<main>
        <h1>Pareto's law and when to stop testing</h1>
        
<blockquote>
  <p>Don't stop 'til you get enough
- <em>Michael Jackson</em></p>
</blockquote>

<p>When is enough testing enough?</p>

<p>Can we stop now?</p>

<p>What reasons could there be for us to continue ... oh, go on, just another hour or so ... please!</p>

<p>Some reasons for stopping testing, from my experience:</p>

<ol>
<li>run out of time</li>
<li>executed all the planned tests</li>
<li>awaiting a new release</li>
<li>stakeholders have agreed to stop based on prevailing risk situation</li>
</ol>

<p>The reason I've been thinking about this is that people have different views of how the Pareto principle applies to testing.</p>

<p>First, we should know what the principle is if we want to analyse how it applies to testing. </p>

<p>The Pareto Principle is otherwise known as the '80/20 rule'.
At its core is a disproportionate relationship between two factors.
It states that 80% of the outcomes come from 20% of the causes.</p>

<p>For example, 80% of the income tax revenue comes from just 20% of the income tax payers.
I make no claim that this is true, by the way, but merely my example.</p>

<p>The underlying message is that things in life are not linear:- 
if you add one more contributor (a cause) you will not reap one more unit of output (the outcome).</p>

<p>Of course, that last statement <em>will</em> actually hold true at some point in a sample.  It could be that the 33rd contributor added will add one more unit, or it could be the 67th.  </p>

<p>Using averages, as we tend to, leads us to think that adding one extra contributor each time will add the same <em>n</em> output each time.  That's how we simplify things in our minds, by working with averages.</p>

<p>The Pareto Principle simply highlights that there are <em>real</em> numbers behind these scenarios (not <em>what</em> those numbers are) rather than the same average being applied to each one.</p>

<p>So, 100 people's salaries may total to £1m.  The average would then be £10,000 (<code>1,000,000 / 100 = 10,000</code>).  Meaning the average salary is £10,000.</p>

<p>The reality will be quite different to this, depending on where you are.  Surveying 100 people's salaries is likely to lead to an 80/20 split.  Meaning that 80% of the total of all the salaries is earned by just 20 people.  Put another way, the 20 highest earning people of the 100 earn 80% of the total salaries.  In some situations we may witness a 90/10 split or a 70/30 split, but the indicative imbalance invites us to look at and think about situations and relationships differently.</p>

<p>The upshot of the Pareto Principle is that, knowing this lopsided arrangement, we could look to optimise for those 20% super-yielding contributors.</p>

<p>For a simple example, if a business sees that 80% of its profits come from just 20% of its products (and not its services), it could seek to put more resources into selling more units of the product, diverting resources from selling its services.  The leverage gained would lead us to believe the company would increase its profit with little or no extra cost of doing so.</p>

<p>Back to <strong>Software Testing</strong> then!  How is the 80/20 rule relevant?</p>

<p>These lopsided relationships are often seen in things like:</p>

<ul>
<li>80% of the bugs found reside in 20% of the software</li>
<li>80% of the bugs are found in 20% of the time spent testing</li>
<li>80% of bugs are found while executing only 20% of the tests</li>
</ul>

<p>If we then assume that these examples are broadly true, how would or should we respond?
What would we do differently now we know that 80% of the time we spend testing we're only going to find a few bugs compared to the first 20%?
Does it even mean this?  Is the Pareto Principle saying this is a chronological relationship?  Or could those super-yielding 20 minutes of testing (out of a total 100 minutes) be dispersed throughout the 100 minutes?   Surely they must be dispersed.  We cannot magically happen upon the most fruitful tests right from the start.  But perhaps we do, when we use risk based testing to inform our priorities.</p>

<p>I will give this deeper thought and write more on it another time.</p>

<p>Over and Out!</p>
</main>
</article>
<article>
<main>
        <h1>QA Testers to gain upper hand over software developers?</h1>
        <p>I read an article recently saying that AI (tools such as GitHub's Copilot) is going to erode the need for software developers while testers will be more in demand than ever!</p>

<p>This sounds inflammatory but there was some merit to the assertion, although life is never really that simple!</p>

<p>It was saying that today where a developer needs 2 days to produce some code, a tester then needs a day to do their QA tasks on it.
Then contrast that with using Copilot, and the same code could be produced in a few hours, while the tester still needs a day for their work.
The extraction then from this is that we'll need more testers than developers to keep up with the work!</p>

<blockquote>
  <p>The article sees the demand for QA people rising along with the salaries.</p>
</blockquote>

<p>This sounds great to the software QA population and champagne corks will soon be popping.
However, as with all life's man-made efficiencies and automation solutions, it usually means we expect even more to be achieved because part of what I did by hand before is now being done quicker by a machine.
It is likely that software developers will be relieved of doing some of these AI-automated tasks but will then have more time to spend on other human-centric work.
This could mean that designs could have more time spent on them or user interfaces could be even more polished, or more emphasis placed on system security or performance.</p>

<p>I see these labour-saving tools just freeing humans up to do more complex work - the work that knowledge workers tend to enjoy and is usually what they signed up for in the first place.</p>

<blockquote>
  <p>AI in software development is akin to test automation in quality assurance</p>
</blockquote>

<p>In this light, AI for developers can be compared to the arrival (and recommended usage) of test automation tools for testers.
Their arrival heralded the end for <em>'manual'</em> testers while the robots take over.
Testers concerned for their livelihood scrambled to learn test automation, or be automated out of a job!</p>

<p>What we have seen, however, is these tools have aided a tester's work, enabling the human to use its cognitive power for complex tasks that a computer cannot perform. </p>

<p>I predict the same happening with AI tools for software engineers.</p>
</main>
</article>
<article>
<main>
        <h1>Software development teams must collaborate, deeply</h1>
        <p>I find, with many virtual concepts, such as computer software, that often people find it difficult to see what's going on and how the system operates.
This then means they are oblivious to things going wrong or that could simply benefit from improvement.
They are unable to see when a teammate needs help or when the output from the system (the product) isn't quite right.</p>

<p>By <em>system</em>, I mean the process, tasks, interactions; the way we organise ourselves in order to produce the software output.</p>

<p>If, for example, we take the production of wooden tables, assuming for now it is done by a team of humans together in a physical workshop, I will try to illustrate my thinking.
4 people working in this way together can see the progress that's been made and which stage we are at.
We can quickly lean across and help each other making legs or fixings or the table top itself.
We can refer back to designs and drawings and hold these up to the individual parts and the whole to see if it looks right.</p>

<p>Imagine if one of us takes an enormous chisel to the finished table top, thinking they'll use it to quickly remove a slight protruding imperfection, and then slips and damages the table.  We would all quickly know what happened.  We would see the damage and will know that large tool caused it - (what an unintentional pun). 
We would know what to do to remedy the situation, which would likely be some kind of wood filler to the gouge and then work to smoothen and finish it off.</p>

<p>Also imagine if one of us is struggling with how to use a particular tool in order to render the desired finish.
The rest of the team would see us using the wrong technique or holding the tool incorrectly.
They could then quickly show us how it's done and we'd be on our way with a new skill.</p>

<p>Now, imagine a team working on something intangible, such as software.
Each individual tends to work on their own, with their own tools (or at least their own instance of the tools), often without any collaboration.
None of their team mates can see anyone struggling with a tool and quickly jump in to help.
If any imperfections arise then the investigation is much more involved than a quick glance at a table and an incorrect tool laying near by.</p>

<p>Working with virtual products, software, is fraught with risks at all turns, and I believe this is largely because we work individually.
Working this way means we are only as good as the sum of our own skills and experiences.</p>

<blockquote>
  <p>Working extensively, individually, means we will be blinkered into thinking we're building the right thing in the right way.</p>
</blockquote>

<p>There are many other places to read about further disadvantages to this way of working and to also find the plus side to it.
For example, on our own we get to focus with no interruptions.  We won't get 'held back' helping those of lower skill in a particular task.</p>

<p>However, I think the pros massively outweigh the cons on this one.</p>

<p>Imagine if a table-making team worked individually.
Think about how things would turn out if they each worked in their own homes, 2 people producing the legs; one working on the table top; and one making the fixings for bolting it all together.  A small 'adjustment' by the leg maker won't be noticed until they next meet to assemble a table.  He may have made 40 such legs by this time.  We may find that the fixings don't match this new version of leg.</p>

<p>Well, this is what often happens in software teams.  We all meet up again (via an integration pipeline) only to find there are clashes and failing tests (if such things exist!). 
Two people came away from a meeting with 2 different understandings of the problem to solve and then went off to solve <em>their</em> version of it. </p>

<blockquote>
  <p>Collaboration helps negate misunderstandings, failure demand, and false implementations</p>
</blockquote>

<p>How do we collaborate in order to reap this particular miracle?
I like to think that when faced with <em>any</em> task there needs to be a good reason why it cannot be done collaboratively.</p>

<p>Thinking of product ideas? - Collaborate!</p>

<p>Creating the design? - Collaborate!</p>

<p>Thinking of acceptance criteria? - Collaborate!</p>

<p>Seeking the cause of a bug? - Collaborate!</p>

<p>Writing the code? - Collaborate!
This can be done using <em>Pairing</em> or <em>Mobbing</em>, for example.</p>

<p>Wanting to improve the team ways of working? - Collaborate!</p>

<p>Locating a Production issue so we can get our service back up and running!? - Collaborate!  Oh, what, you do that already as part of your incident response?
That's interesting!  When something is the most crucial and critical to the business, you gather people together and pull out all the stops to get it done.
Why not do that all the time and benefit from it on everything?</p>

<p>Perhaps this is the <a href="paretos-law-and-when-to-stop-testing.html">80/20 rule</a> at play.
Collaborating over incidents is giving us the 80% upside - we're at our most effective and efficient, however we only do this 20% of the time (well, much less if we're talking incidents, but you get the point!). </p>
</main>
</article>
<article>
<main>
        <h1>Testing role requirements impossible to fulfil</h1>
        <p>Back in the day, there was a distinct hierarchy to software testing jobs.
I'm talking about the 1990's when we had Test Analysts (in addition to Junior and Senior versions thereof).
You could then move up to being a Test Manager, if you worked on planned, waterfall, types of projects.
If you then progressed to overseeing organisational Testing or Quality Assurance then you could go for QA Manager roles.
There was a clear progression path and distinct roles.
QA Managers would not normally be expected to do any testing, and would line-manage their team of testers.</p>

<p>As time went by we saw the need for automation skills for a tester, and it was usually to automate regression tests.
Tools for this were off the shelf capture replay tools, such as Mercury Winrunner.
You could record some clicks through some software and the tool would churn out some code, like a macro.
This code could then be embellished to make it as reusable as you wanted.</p>

<p>During this period we saw the rise of mass software adoption and the need for performant systems was born.
Hence, performance testing tools were introduced, such as Mercury LoadRunner.</p>

<p>Again, tehcnical testers doing automation or performance testing were not required to do management; that was the Test Manager's or QA Manager's role.</p>

<p>The issue we have now is that the proliferation of iterative software development has largely negated the need for leaders or managers in  QA / testing.
We want <strong>doers</strong> and not <em>managers</em>.</p>

<p>This would all be fine if organisations ran their software development using collaborative teams using Continuous Integration and all the automated tests that this requires.
What we find though is that teams adopt the agile events, such as the stand-up, and organise themselves in cross functional units, but they still leave the testing to QA / test people.
Ultimately, we have ended up with just as many QA people as before but with no QA leadership or management.
To address this, companies now look for a person who can manage and set testing strategies in addition to writing automation frameworks, and everything in between.</p>

<p>In short, we thought we didn't need testers.
Then we realised we did, and built teams of them everywhere.
Then we went agile and got rid of test management and lots of testers.
Now testers are coming back but we don't want to pay for the management of them!</p>
</main>
</article>
<article>
<main>
        <h1>Using bash to create a very simple blog site</h1>
        <p>So, what I did was create a very very simple website based on Bootsrap CSS.</p>
        <p>It has a homepage and a blog page.  The blog page just has links to blog posts.  Each blog post has links to all blog posts at the bottom after the blog content.  That's it.  In order though to easily create a new blog post I wrote a bash script that adds the before, after, and recent blogs html code.  That's it really.  Enjoy!</p>
</main>
</article>
</body>
</html>
